# -*- coding: utf-8 -*-
"""Copy of MIT6.036 hw06 colab notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ZTyUBGUO7XqQLf5HhaQNsRWHkGEtLa_

# MIT 6.036 Spring 2019: Homework 6

This homework does not include provided Python code. Instead, we encourage you to write your own code to help you answer some of these problems, and/or test and debug the code components we do ask for. All of the problems should be simple enough that hand calculation should be possible, but it may be convenient to write some short programs to explore the neural networks, particularly for problem 2.
"""

import numpy as np

def SM(z):
  # input z: n by 1 vector
  # implement softmax
  n=len(z)
  a=np.zeros([n,1])
  sum_e=np.sum([np.exp(z[j]) for j in range(n)])
  for j in range(n):
    a[j]=np.exp(z[j])/sum_e
    
  return a



"""**Problem 2A**"""

z = np.array([[-1, 0, 1]]).T
a=SM(z)

"""**Problem 2.C-F**"""

w = np.array([[1, -1, -2], [-1, 2, 1]])
x = np.array([[1], [1]])
y = np.array([[0, 1, 0]]).T

# 2.c
z=w.T@x
a=SM(z)
grad=x@(a-y).T
new_w=w.astype(float)+0.5*grad
new_w

# 2.E
new_w=w-0.5*grad

# 2.F
new_a=SM(new_w.T@x)
new_a

"""**Problem 3**"""

# layer 1 weights
w_1 = np.array([[1, 0, -1, 0], [0, 1, 0, -1]])
w_1_bias = np.array([[-1, -1, -1, -1]]).T
# layer 2 weights
w_2 = np.array([[1, -1], [1, -1], [1, -1], [1, -1]])
w_2_bias = np.array([[0, 2]]).T

# your code here

def ReLU(z):
  # z is a n by 1 vector
  n=len(z)
  result=np.zeros([n,1])
  for i in range(n):
    result[i]=max(0,z[i])
    
  return result

x=np.array([[0.5,0,-3],[0.5,2,0.5]])

z=w_1.T@x+np.hstack((w_1_bias,w_1_bias,w_1_bias))
ReLU(z[:,2])

np.hstack((w_1_bias,w_1_bias,w_1_bias)).shape

w_1_bias

z

